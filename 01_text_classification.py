# -*- coding: utf-8 -*-
"""01_text_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QBeaBnY3PH9tOKvG-T8X2L_8qWR2RDeI

## **Sentimental Analysis of Movie Reviews using**

Representational and


1.  Phi-3
2.  BERT

Generative Models

3.  FLAN T-5
4.  Chat GPT

### Text Generation with Representational Models

#### Microsoft Phi-3
"""

! pip install transformers>=4.40.1 accelerate>=0.27.2

from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(
    "microsoft/Phi-3-mini-4k-instruct",
    device_map="cuda",
    dtype="auto",
    #trust_remote_code=True,
)

tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-4k-instruct")

print(f"Vocabulary size: {tokenizer.vocab_size}")

# Test tokenizer with some examples

words = ["Hello", "world", "AI", "learning"]
for word in words:
  print(f"Tokenized {word}: {tokenizer.encode(word)}")


# Test the tokenizer with a sentence
sentence = "Artificial Intelligence is transforming the world"
encoded_sentence = tokenizer.encode(sentence)
decoded_sentence = tokenizer.decode(encoded_sentence)

print(f"\nOriginal sentence: {sentence}")
print(f"Encoded sentence: {encoded_sentence}")
print(f"Decoded sentence: {decoded_sentence}")

encoded_sentence = tokenizer.encode(sentence)

tokens = [tokenizer.decode([token_id]) for token_id in encoded_sentence]

# Create map of token ids to tokens
token_id_map = list(zip(encoded_sentence, tokens))

for token_id, token in token_id_map :
  print(f"Token ID:{token_id}, Token: '{token}'")

from transformers import pipeline

generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    return_full_text=False,
    max_new_tokens=100,
    do_sample=False)

# The prompt
messages = [
    {"role": "system", "content": "You are a standup comedian"},
    {"role": "user", "content": "Create a funny joke about chickens"}
]

output = generator(messages)
print(output[0]["generated_text"])



"""### **Text Classification with Representation Models**

#### 1. BERT - RobertaTwitter

0. Install libraries
"""

!pip install datasets transformers sentence-transformers openai

"""1. Import data"""

from datasets import load_dataset

data = load_dataset("rotten_tomatoes")
data

data["train"][0, -1]

"""Using a Task-specific model"""

from transformers import pipeline

model_path = "cardiffnlp/twitter-roberta-base-sentiment-latest"

pipe = pipeline(
    model=model_path,
    tokenizer=model_path,
    return_all_scores=True,
    device="cuda:0"
)

import numpy as np
from tqdm import tqdm
from transformers.pipelines.pt_utils import KeyDataset

# Run inference
y_pred = []
for output in tqdm(pipe(KeyDataset(data["test"], "text")), total=len(data["test"])):
  negative_score = output[0]["score"]
  positive_score = output[2]["score"]
  assignment = np.argmax([negative_score, positive_score])
  y_pred.append(assignment)

print(y_pred[1])
print(y_pred[36])
print(y_pred[1000])

"""Evaluate"""

from sklearn.metrics import classification_report

def evaluate_perfomance(y_true, y_pred):
  perfomance = classification_report(
      y_true, y_pred,
      target_names=["negative review", "positive review"])
  print(perfomance)

evaluate_perfomance(data["test"]["label"], y_pred)

"""### **Text Classification with Generative Models**

#### FLAN T5
"""

!pip install datasets transformers sentence-transformers openai

from datasets import load_dataset

data = load_dataset("rotten_tomatoes")
data

"""
**Enocder-Decoder model**
"""

from transformers import pipeline

pipe = pipeline(
    'text2text-generation',
    model="google/flan-t5-small",
    device="cuda:0"
)

prompt = "Is the following sentence positive or negative?"
data = data.map(lambda example: {"t5": prompt + example['text']})
data

data["train"]["t5"][0]

import numpy as np
from tqdm import tqdm
from transformers.pipelines.pt_utils import KeyDataset

y_pred = []
for output in tqdm(pipe(KeyDataset(data["test"], "t5")), total=len(data["test"])):
  text = output[0]["generated_text"]
  y_pred.append(0 if text == "negative" else 1)

from sklearn.metrics import classification_report

def evaluate_perfomance(y_true, y_pred):
  perfomance = classification_report(
      y_true, y_pred,
      target_names=["Negative Review", "Postive Review"]
  )
  print(perfomance)

evaluate_perfomance(data["test"]["label"], y_pred)

"""#### Chat GPT for Classification"""

!pip install datasets openai

from datasets import load_dataset

data = load_dataset("rotten_tomatoes")
data

import openai

client = openai.OpenAI(api_key="")

def chatgpt_generation(prompt, document, model="gpt-3.5-turbo"):
  messages = [
      {
          "role": "system",
          "content": "You are a helpful assistant."
        },
      {
          "role": "user",
          "content": prompt.replace("[DOCUMENT]", document)
        }

      ]

  chat_completion = client.chat.completions.create(
          messages=messages,
          model=model,
          temperature=0
      )
  return chat_completion.choices[0].message.content

prompt = """Predict whethe the following document is a positive or negative movie review

Document:
If it is positive return 1 and if it is negative return 0. Do not give any other answers.
"""

document = "unpretentious, charming, quirky, original"
chatgpt_generation(prompt, document)

import numpy as np
import tqdm as tqdm

predictions = [chatgpt_generation(prompt, document) for document in tqdm(data["test"][ "text"])]

from sklearn.metrics import classification_report

def evaluate_perfomance(y_true, y_pred):
  perfomance = classification_report(
      y_true, y_pred,
      target_names=["Negative Review", "Postive Review"]
  )
  print(perfomance)

y_pred = [int(pred) for pred in predictions]

evaluate_perfomance(data["test"]["label"], y_pred)

